25/06/29 20:17:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/06/29 20:17:58 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-81-205.ec2.internal/172.31.81.205:8032
25/06/29 20:17:59 INFO Configuration: resource-types.xml not found
25/06/29 20:17:59 INFO ResourceUtils: Unable to find 'resource-types.xml'.
25/06/29 20:17:59 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
25/06/29 20:17:59 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead
25/06/29 20:17:59 INFO Client: Setting up container launch context for our AM
25/06/29 20:17:59 INFO Client: Setting up the launch environment for our AM container
25/06/29 20:17:59 INFO Client: Preparing resources for our AM container
25/06/29 20:17:59 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
25/06/29 20:18:02 INFO Client: Uploading resource file:/mnt/tmp/spark-d635b4ae-684f-4a36-95b2-a6381ada7a4e/__spark_libs__3998059195340573718.zip -> hdfs://ip-172-31-81-205.ec2.internal:8020/user/hadoop/.sparkStaging/application_1751228211776_0001/__spark_libs__3998059195340573718.zip
25/06/29 20:18:03 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-81-205.ec2.internal:8020/user/hadoop/.sparkStaging/application_1751228211776_0001/hive-site.xml
25/06/29 20:18:03 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-81-205.ec2.internal:8020/user/hadoop/.sparkStaging/application_1751228211776_0001/hudi-defaults.conf
25/06/29 20:18:05 INFO ClientConfigurationFactory: Set initial getObject socket timeout to 2000 ms.
25/06/29 20:18:05 INFO Client: Uploading resource s3://glue-hospital-data/scripts/run_emr_merge_job.py -> hdfs://ip-172-31-81-205.ec2.internal:8020/user/hadoop/.sparkStaging/application_1751228211776_0001/run_emr_merge_job.py
25/06/29 20:18:07 INFO Client: Deleted staging directory hdfs://ip-172-31-81-205.ec2.internal:8020/user/hadoop/.sparkStaging/application_1751228211776_0001
Exception in thread "main" java.io.FileNotFoundException: No such file or directory 's3://glue-hospital-data/scripts/run_emr_merge_job.py'
	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:560)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:623)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:429)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:368)
	at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:461)
	at org.apache.spark.deploy.yarn.Client.distribute$1(Client.scala:557)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:728)
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:984)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:221)
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1322)
	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1770)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1066)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1158)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1167)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
25/06/29 20:18:07 INFO ShutdownHookManager: Shutdown hook called
25/06/29 20:18:07 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-d635b4ae-684f-4a36-95b2-a6381ada7a4e
25/06/29 20:18:07 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-a4144b7a-2276-4911-85d2-890bf01e168a
Command exiting with ret '1'
